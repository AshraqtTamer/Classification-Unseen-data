# -*- coding: utf-8 -*-
"""TicketClassification_testing unseen data(2 datasets).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRJPULhtYMEt5r3Xxc0WjfUOL2Uf4j20

# Load Dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip install datasets

"""# First dataset"""

data=pd.read_csv('/content/dataset-tickets-multi-lang-4-20k.csv')
data.head()

data = data[data['language'] != 'de']
data.head()

len(data)

data_selected = data[['body', 'type']]
data_selected.head()

data_selected.value_counts('type')

data_selected.isnull().sum()

data_selected.dropna(inplace=True)
data_selected.isnull().sum()

data_selected.duplicated().sum()

data_selected['type'] = data_selected['type'].replace({
    'Change': 'Change/Request',
    'Request': 'Change/Request'
})
data_selected.value_counts('type')

data_selected['type'] = data_selected['type'].replace({
    'Incident': 'Incident/Problem',
    'Problem': 'Incident/Problem'
})

data_selected.value_counts('type')
data_selected['type'] = data_selected['type'].replace({
    'Incident/Problem': 'Problem',
    'Change/Request': 'Request'
})

data_selected.value_counts('type')

data_selected.head()

data_selected.to_csv('data_selected.csv', index=False)

"""# Second dataset"""

from datasets import load_dataset

ds = load_dataset("gorkemsevinc/customer_support_tickets", split='train')
import pandas as pd
df = pd.DataFrame(ds)
df

df['Ticket Type']= df['Ticket Type'].replace({
    'refund request': 'Request',
    'technical issue': 'Problem',
    'cancellation request': 'Request',
    'product inquiry': 'Request',
    'billing inquiry': 'Request'
})
df.head()

data_selected_second= df[['Combined Text', 'Ticket Type']]
data_selected_second.head()

data_selected_second.value_counts('Ticket Type')

data1 = data_selected[['body', 'type']]
data2 = data_selected_second[['Combined Text', 'Ticket Type']]

# Rename columns to have uniform names for merging
data1 = data1.rename(columns={'body': 'Text', 'type': 'Category'})
data2 = data2.rename(columns={'Combined Text': 'Text', 'Ticket Type': 'Category'})

# Concatenate both datasets
final_combined_data = pd.concat([data1, data2], ignore_index=True)

# Display the first few rows
final_combined_data.head()

len(final_combined_data)

download_data = final_combined_data.to_csv('final_combined_data.csv', index=False)

"""# Text Preprocessing"""

import re
import string
import spacy
nlp = spacy.load('en_core_web_sm')

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import numpy as np
import pandas as pd
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    tokens = text.split()  # Tokenization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatization
    return ' '.join(tokens)

def clean_text(text):
    text = text.lower()  # Make the text lowercase
    text = re.sub('\[.*\]','', text).strip() # Remove text in square brackets
    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation
    text = re.sub('\S*\d\S*\s*','', text).strip()  # Remove words containing numbers
    return text.strip()

final_combined_data['Text'] = final_combined_data['Text'].apply(clean_text)

stopwords = nlp.Defaults.stop_words
def lemmatizer(text):
    doc = nlp(text)
    sent = [token.lemma_ for token in doc if not token.text in set(stopwords)]
    return ' '.join(sent)

final_combined_data['Text'] = final_combined_data['Text'].apply(lambda x: lemmatizer(x))

final_combined_data.head()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Feature extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')
dtm= tfidf.fit_transform(final_combined_data['Text'])
dtm

tfidf.get_feature_names_out()[:10]

final_combined_data

plt.figure(figsize=(12,6))
sns.countplot(x='Category',data=final_combined_data)

from collections import Counter

from imblearn.over_sampling import SMOTE

X = dtm
y = final_combined_data['Category']


smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)


print("Original dataset shape %s" % Counter(y))
print("Resampled dataset shape %s" % Counter(y_resampled))

X_resampled.shape, y_resampled.shape

plt.figure(figsize=(12,6))
sns.countplot(x=y_resampled)

"""# Model Training"""

from sklearn.feature_extraction.text import CountVectorizer
x=X_resampled
y=y_resampled
count_vec=CountVectorizer()

X_text = [tfidf.inverse_transform(row)[0] for row in x]

X_text = [' '.join(doc) for doc in X_text]
X_vect = count_vec.fit_transform(X_text)

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_vect)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

"""# Classification models

# MultinomialNB
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

clf = MultinomialNB()
clf.fit(X_train, y_train)


y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

"""# XGBOOST Classifier"""

!pip install xgboost
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix

# ... (Your existing code) ...

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable
y_encoded = label_encoder.fit_transform(y)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

# Initialize XGBoost classifier
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_encoded)), random_state=42)

# Train the classifier
xgb_classifier.fit(X_train, y_train)

# Make predictions
y_pred = xgb_classifier.predict(X_test)

# # Inverse transform predictions to original labels if needed
# y_pred_original = label_encoder.inverse_transform(y_pred)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test)) # You might want to use label_encoder.classes_ here for original labels
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# SVM Classifiers"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder (if not already initialized)
label_encoder = LabelEncoder()

# Fit and transform the target variable (if not already done)
y_encoded = label_encoder.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

# Initialize SVM classifier
svm_classifier = SVC(random_state=42)  # You can adjust parameters as needed

# Train the classifier
svm_classifier.fit(X_train, y_train)

# Make predictions
y_pred = svm_classifier.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# SKFold"""

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder outside the loop
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Encode labels before the loop

# Initialize models
clf = MultinomialNB()
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_encoded)), random_state=42)

models = [clf, xgb_classifier]
model_names = ['MultinomialNB', 'XGBoost']

# Initialize StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model, model_name in zip(models, model_names):
    print(f"Evaluating {model_name}:")
    accuracy_scores = []
    for fold, (train_index, test_index) in enumerate(skf.split(x, y_encoded)):  # Use y_encoded for splitting
        X_train, X_test = x[train_index], x[test_index]
        y_train, y_test = y_encoded[train_index], y_encoded[test_index]  # Use encoded labels

        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)
        print(f"Fold {fold + 1}: Accuracy = {accuracy}")
        print(classification_report(y_test, y_pred))

    avg_accuracy = np.mean(accuracy_scores)
    print(f"Average Accuracy for {model_name}: {avg_accuracy}")
    print("-" * 30)

"""# Test predictions"""

import pandas as pd

new_text = "I’ve been lockedAll my saved data appears to be wiped after the recent update, and I rely on this for daily work logs."
new_text = clean_text(new_text)
# new_text = lemmatizer(new_text)
# new_text = extract_pos_tags(new_text)

# Transform the new text using the fitted TF-IDF vectorizer
new_text_dtm = tfidf.transform([new_text])

# Use CountVectorizer and TF-IDF Transformer on the new text
new_text_vect = count_vec.transform([new_text])
new_text_tfidf = tfidf_transformer.transform(new_text_vect)

# Predict using the trained model (choose the model you want to use for prediction)

# Example with MultinomialNB:
predicted_category_nb = clf.predict(new_text_tfidf)[0]
predicted_category_nb = label_encoder.inverse_transform([predicted_category_nb])[0]
print(f"Predicted category (MultinomialNB): {predicted_category_nb}")

# Example with SVC
predicted_category_svm = svm_classifier.predict(new_text_dtm)[0]
predicted_category_svm = label_encoder.inverse_transform([predicted_category_svm])[0]
print(f"Predicted category (SVM): {predicted_category_svm}")

# Example with XGBoost
predicted_category_xgb = xgb_classifier.predict(new_text_dtm)[0]
# Convert back to the original label:
predicted_category_xgb = label_encoder.inverse_transform([predicted_category_xgb])[0]
print(f"Predicted category (XGBoost): {predicted_category_xgb}")

import pandas as pd

# 🧪 20 Test messages and expected outputs
test_messages = [
    "Can someone help me set up my new account?",
    "I'm seeing an error message every time I try to log in.",
    "Is it possible to upgrade my plan to the premium tier?",
    "My order hasn’t arrived even though it’s marked as delivered.",
    "I’d like to request a change to my delivery address.",
    "The app keeps crashing after the recent update.",
    "Can I get a list of all previous invoices for this year?",
    "I was promised a discount but was charged the full amount.",
    "How do I reset my PIN for security reasons?",
    "The tracking number provided doesn’t work on your website.",
    "Please confirm if my subscription has been successfully canceled.",
    "The payment page is stuck and won’t let me proceed.",
    "Can I postpone my subscription renewal for one month?",
    "The system keeps logging me out after just a few minutes.",
    "I’d like to know the refund policy for canceled events.",
    "None of my messages are being delivered through the chat feature.",
    "Can I change the registered email on my account?",
    "The two-factor authentication code isn’t arriving.",
    "I’d like to request training on how to use the dashboard.",
    "My account was suspended without any notice."
]

expected_labels = [
    "Request", "Problem", "Request", "Problem", "Request",
    "Problem", "Request", "Problem", "Request", "Problem",
    "Request", "Problem", "Request", "Problem", "Request",
    "Problem", "Request", "Problem", "Request", "Problem"
]

# Store results
results = []

for i, msg in enumerate(test_messages):
    # Preprocess
    clean = clean_text(msg)
    # clean = lemmatizer(clean)        # Optional if you're using it
    # clean = extract_pos_tags(clean)  # Optional if you're using it

    # Transform for prediction
    dtm = tfidf.transform([clean])
    vect = count_vec.transform([clean])
    tfidf_input = tfidf_transformer.transform(vect)

    # Predict
    pred_nb = clf.predict(tfidf_input)[0]
    pred_svm = svm_classifier.predict(dtm)[0]
    pred_xgb = xgb_classifier.predict(dtm)[0]

    # Decode
    pred_nb = label_encoder.inverse_transform([pred_nb])[0]
    pred_svm = label_encoder.inverse_transform([pred_svm])[0]
    pred_xgb = label_encoder.inverse_transform([pred_xgb])[0]

    # Save
    results.append({
        "Message": msg,
        "Expected": expected_labels[i],
        "MultinomialNB": pred_nb,
        "SVM": pred_svm,
        "XGBoost": pred_xgb
    })

# Create DataFrame for output
df_results = pd.DataFrame(results)

# Show the result
print(df_results)
df_results.to_csv("model_comparison_results.csv", index=False)

"""# With Auto response model"""

import random
import string


# Function to generate a random ticket ID
def generate_ticket_id():
    ticket_id = ''.join(random.choices(string.digits, k=8))
    return ticket_id

# Function to generate auto-response based on the predicted class
def auto_response(text, user_name, ticket_subject, email):
    # Automatically generate ticket ID
    ticket_id = generate_ticket_id()

    # Predict the class of the ticket (e.g., "Request", "Problem", etc.)
    predicted_class = predict_text(text)
    response_templates = {
        "Problem": f"""
        Dear {user_name},

        We understand that you're facing an issue with '{ticket_subject}'.

        Your ticket ID is #{ticket_id}. Our support team is looking into your issue and will get back to you as soon as possible.

        Regards,
        Support Team
        """,
        "Request": f"""
        Dear {user_name},

        Thank you for submitting a request regarding '{ticket_subject}'.

        Your request has been received with Ticket ID #{ticket_id}. Our team is processing your request and will update you shortly.

        Regards,
        Support Team
        """
    }

    default_response = f"""
    Dear {user_name},

    We have received your inquiry regarding '{ticket_subject}'.

    Our team will review your message and get back to you shortly.

    Regards,
    Support Team
    """

    response_text = response_templates.get(predicted_class, default_response)

    return response_text


input_text = input("Input text description: ")
user_name = input("Enter user's name: ")
ticket_subject = input("Enter ticket subject: ")
email = input("Enter user's email: ")

print("\nAuto-Response:\n")
print(auto_response(input_text, user_name, ticket_subject, email))

