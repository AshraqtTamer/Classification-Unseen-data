# -*- coding: utf-8 -*-
"""TicketClassification_testing unseen data(2 datasets).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1IRJPULhtYMEt5r3Xxc0WjfUOL2Uf4j20

# Load Dataset
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

!pip install datasets

"""# First dataset"""

data=pd.read_csv('/content/dataset-tickets-multi-lang-4-20k.csv')
data.head()

data = data[data['language'] != 'de']
data.head()

len(data)

data_selected = data[['body', 'type']]
data_selected.head()

data_selected.value_counts('type')

data_selected.isnull().sum()

data_selected.dropna(inplace=True)
data_selected.isnull().sum()

data_selected.duplicated().sum()

data_selected['type'] = data_selected['type'].replace({
    'Change': 'Change/Request',
    'Request': 'Change/Request'
})
data_selected.value_counts('type')

data_selected['type'] = data_selected['type'].replace({
    'Incident': 'Incident/Problem',
    'Problem': 'Incident/Problem'
})

data_selected.value_counts('type')
data_selected['type'] = data_selected['type'].replace({
    'Incident/Problem': 'Problem',
    'Change/Request': 'Request'
})

data_selected.value_counts('type')

data_selected.head()

data_selected.to_csv('data_selected.csv', index=False)

"""# Second dataset"""

from datasets import load_dataset

ds = load_dataset("gorkemsevinc/customer_support_tickets", split='train')
import pandas as pd
df = pd.DataFrame(ds)
df

df['Ticket Type']= df['Ticket Type'].replace({
    'refund request': 'Request',
    'technical issue': 'Problem',
    'cancellation request': 'Request',
    'product inquiry': 'Request',
    'billing inquiry': 'Request'
})
df.head()

data_selected_second= df[['Combined Text', 'Ticket Type']]
data_selected_second.head()

data_selected_second.value_counts('Ticket Type')

data1 = data_selected[['body', 'type']]
data2 = data_selected_second[['Combined Text', 'Ticket Type']]

# Rename columns to have uniform names for merging
data1 = data1.rename(columns={'body': 'Text', 'type': 'Category'})
data2 = data2.rename(columns={'Combined Text': 'Text', 'Ticket Type': 'Category'})

# Concatenate both datasets
final_combined_data = pd.concat([data1, data2], ignore_index=True)

# Display the first few rows
final_combined_data.head()

len(final_combined_data)

download_data = final_combined_data.to_csv('final_combined_data.csv', index=False)

"""# Text Preprocessing"""

import re
import string
import spacy
nlp = spacy.load('en_core_web_sm')

import re
import nltk
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
import numpy as np
import pandas as pd
nltk.download('stopwords')
nltk.download('wordnet')

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))
def clean_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'\W+', ' ', text)  # Remove special characters
    tokens = text.split()  # Tokenization
    tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]  # Lemmatization
    return ' '.join(tokens)

def clean_text(text):
    text = text.lower()  # Make the text lowercase
    text = re.sub('\[.*\]','', text).strip() # Remove text in square brackets
    text = text.translate(str.maketrans('', '', string.punctuation)) # Remove punctuation
    text = re.sub('\S*\d\S*\s*','', text).strip()  # Remove words containing numbers
    return text.strip()

final_combined_data['Text'] = final_combined_data['Text'].apply(clean_text)

stopwords = nlp.Defaults.stop_words
def lemmatizer(text):
    doc = nlp(text)
    sent = [token.lemma_ for token in doc if not token.text in set(stopwords)]
    return ' '.join(sent)

final_combined_data['Text'] = final_combined_data['Text'].apply(lambda x: lemmatizer(x))

final_combined_data.head()

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

"""# Feature extraction"""

from sklearn.feature_extraction.text import TfidfVectorizer
tfidf = TfidfVectorizer(min_df=2, max_df=0.95, stop_words='english')
dtm= tfidf.fit_transform(final_combined_data['Text'])
dtm

tfidf.get_feature_names_out()[:10]

final_combined_data

plt.figure(figsize=(12,6))
sns.countplot(x='Category',data=final_combined_data)

from collections import Counter

from imblearn.over_sampling import SMOTE

X = dtm
y = final_combined_data['Category']


smote = SMOTE(random_state=42)
X_resampled, y_resampled = smote.fit_resample(X, y)


print("Original dataset shape %s" % Counter(y))
print("Resampled dataset shape %s" % Counter(y_resampled))

X_resampled.shape, y_resampled.shape

plt.figure(figsize=(12,6))
sns.countplot(x=y_resampled)

"""# Model Training"""

from sklearn.feature_extraction.text import CountVectorizer
x=X_resampled
y=y_resampled
count_vec=CountVectorizer()

X_text = [tfidf.inverse_transform(row)[0] for row in x]

X_text = [' '.join(doc) for doc in X_text]
X_vect = count_vec.fit_transform(X_text)

from sklearn.feature_extraction.text import TfidfTransformer

tfidf_transformer = TfidfTransformer()
X_tfidf = tfidf_transformer.fit_transform(X_vect)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.2, random_state=42)

"""# Classification models

# MultinomialNB
"""

from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import classification_report, accuracy_score

clf = MultinomialNB()
clf.fit(X_train, y_train)


y_pred = clf.predict(X_test)

print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

"""# XGBOOST Classifier"""

!pip install xgboost
import xgboost as xgb
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import confusion_matrix

# ... (Your existing code) ...

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder
label_encoder = LabelEncoder()

# Fit and transform the target variable
y_encoded = label_encoder.fit_transform(y)


# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

# Initialize XGBoost classifier
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_encoded)), random_state=42)

# Train the classifier
xgb_classifier.fit(X_train, y_train)

# Make predictions
y_pred = xgb_classifier.predict(X_test)

# # Inverse transform predictions to original labels if needed
# y_pred_original = label_encoder.inverse_transform(y_pred)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=np.unique(y_test), yticklabels=np.unique(y_test)) # You might want to use label_encoder.classes_ here for original labels
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# SVM Classifiers"""

from sklearn.svm import SVC
from sklearn.metrics import confusion_matrix

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder (if not already initialized)
label_encoder = LabelEncoder()

# Fit and transform the target variable (if not already done)
y_encoded = label_encoder.fit_transform(y)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(x, y_encoded, test_size=0.2, random_state=42)

# Initialize SVM classifier
svm_classifier = SVC(random_state=42)  # You can adjust parameters as needed

# Train the classifier
svm_classifier.fit(X_train, y_train)

# Make predictions
y_pred = svm_classifier.predict(X_test)

# Evaluate the model
print(classification_report(y_test, y_pred))
print(f"Accuracy: {accuracy_score(y_test, y_pred)}")

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues",
            xticklabels=label_encoder.classes_, yticklabels=label_encoder.classes_)
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.title("Confusion Matrix")
plt.show()

"""# SKFold"""

from sklearn.model_selection import StratifiedKFold
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix
import numpy as np
from sklearn.preprocessing import LabelEncoder

# Assuming X_resampled and y_resampled are your features and labels
x = X_resampled
y = y_resampled

# Initialize LabelEncoder outside the loop
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)  # Encode labels before the loop

# Initialize models
clf = MultinomialNB()
xgb_classifier = xgb.XGBClassifier(objective='multi:softmax', num_class=len(np.unique(y_encoded)), random_state=42)

models = [clf, xgb_classifier]
model_names = ['MultinomialNB', 'XGBoost']

# Initialize StratifiedKFold
skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

for model, model_name in zip(models, model_names):
    print(f"Evaluating {model_name}:")
    accuracy_scores = []
    for fold, (train_index, test_index) in enumerate(skf.split(x, y_encoded)):  # Use y_encoded for splitting
        X_train, X_test = x[train_index], x[test_index]
        y_train, y_test = y_encoded[train_index], y_encoded[test_index]  # Use encoded labels

        # Fit the model
        model.fit(X_train, y_train)

        # Make predictions
        y_pred = model.predict(X_test)

        # Evaluate the model
        accuracy = accuracy_score(y_test, y_pred)
        accuracy_scores.append(accuracy)
        print(f"Fold {fold + 1}: Accuracy = {accuracy}")
        print(classification_report(y_test, y_pred))

    avg_accuracy = np.mean(accuracy_scores)
    print(f"Average Accuracy for {model_name}: {avg_accuracy}")
    print("-" * 30)

"""# Test predictions"""

import pandas as pd

new_text = "Iâ€™ve been lockedAll my saved data appears to be wiped after the recent update, and I rely on this for daily work logs."
new_text = clean_text(new_text)
# new_text = lemmatizer(new_text)
# new_text = extract_pos_tags(new_text)

# Transform the new text using the fitted TF-IDF vectorizer
new_text_dtm = tfidf.transform([new_text])

# Use CountVectorizer and TF-IDF Transformer on the new text
new_text_vect = count_vec.transform([new_text])
new_text_tfidf = tfidf_transformer.transform(new_text_vect)

# Predict using the trained model (choose the model you want to use for prediction)

# Example with MultinomialNB:
predicted_category_nb = clf.predict(new_text_tfidf)[0]
predicted_category_nb = label_encoder.inverse_transform([predicted_category_nb])[0]
print(f"Predicted category (MultinomialNB): {predicted_category_nb}")

# Example with SVC
predicted_category_svm = svm_classifier.predict(new_text_dtm)[0]
predicted_category_svm = label_encoder.inverse_transform([predicted_category_svm])[0]
print(f"Predicted category (SVM): {predicted_category_svm}")

# Example with XGBoost
predicted_category_xgb = xgb_classifier.predict(new_text_dtm)[0]
# Convert back to the original label:
predicted_category_xgb = label_encoder.inverse_transform([predicted_category_xgb])[0]
print(f"Predicted category (XGBoost): {predicted_category_xgb}")

import pandas as pd

# ðŸ§ª 20 Test messages and expected outputs
test_messages = [
    "Can someone help me set up my new account?",
    "I'm seeing an error message every time I try to log in.",
    "Is it possible to upgrade my plan to the premium tier?",
    "My order hasnâ€™t arrived even though itâ€™s marked as delivered.",
    "Iâ€™d like to request a change to my delivery address.",
    "The app keeps crashing after the recent update.",
    "Can I get a list of all previous invoices for this year?",
    "I was promised a discount but was charged the full amount.",
    "How do I reset my PIN for security reasons?",
    "The tracking number provided doesnâ€™t work on your website.",
    "Please confirm if my subscription has been successfully canceled.",
    "The payment page is stuck and wonâ€™t let me proceed.",
    "Can I postpone my subscription renewal for one month?",
    "The system keeps logging me out after just a few minutes.",
    "Iâ€™d like to know the refund policy for canceled events.",
    "None of my messages are being delivered through the chat feature.",
    "Can I change the registered email on my account?",
    "The two-factor authentication code isnâ€™t arriving.",
    "Iâ€™d like to request training on how to use the dashboard.",
    "My account was suspended without any notice."
]

expected_labels = [
    "Request", "Problem", "Request", "Problem", "Request",
    "Problem", "Request", "Problem", "Request", "Problem",
    "Request", "Problem", "Request", "Problem", "Request",
    "Problem", "Request", "Problem", "Request", "Problem"
]

# Store results
results = []

for i, msg in enumerate(test_messages):
    # Preprocess
    clean = clean_text(msg)
    # clean = lemmatizer(clean)        # Optional if you're using it
    # clean = extract_pos_tags(clean)  # Optional if you're using it

    # Transform for prediction
    dtm = tfidf.transform([clean])
    vect = count_vec.transform([clean])
    tfidf_input = tfidf_transformer.transform(vect)

    # Predict
    pred_nb = clf.predict(tfidf_input)[0]
    pred_svm = svm_classifier.predict(dtm)[0]
    pred_xgb = xgb_classifier.predict(dtm)[0]

    # Decode
    pred_nb = label_encoder.inverse_transform([pred_nb])[0]
    pred_svm = label_encoder.inverse_transform([pred_svm])[0]
    pred_xgb = label_encoder.inverse_transform([pred_xgb])[0]

    # Save
    results.append({
        "Message": msg,
        "Expected": expected_labels[i],
        "MultinomialNB": pred_nb,
        "SVM": pred_svm,
        "XGBoost": pred_xgb
    })

# Create DataFrame for output
df_results = pd.DataFrame(results)

# Show the result
print(df_results)
df_results.to_csv("model_comparison_results.csv", index=False)

"""# With Auto response model"""

import random
import string


# Function to generate a random ticket ID
def generate_ticket_id():
    ticket_id = ''.join(random.choices(string.digits, k=8))
    return ticket_id

# Function to generate auto-response based on the predicted class
def auto_response(text, user_name, ticket_subject, email):
    # Automatically generate ticket ID
    ticket_id = generate_ticket_id()

    # Predict the class of the ticket (e.g., "Request", "Problem", etc.)
    predicted_class = predict_text(text)
    response_templates = {
        "Problem": f"""
        Dear {user_name},

        We understand that you're facing an issue with '{ticket_subject}'.

        Your ticket ID is #{ticket_id}. Our support team is looking into your issue and will get back to you as soon as possible.

        Regards,
        Support Team
        """,
        "Request": f"""
        Dear {user_name},

        Thank you for submitting a request regarding '{ticket_subject}'.

        Your request has been received with Ticket ID #{ticket_id}. Our team is processing your request and will update you shortly.

        Regards,
        Support Team
        """
    }

    default_response = f"""
    Dear {user_name},

    We have received your inquiry regarding '{ticket_subject}'.

    Our team will review your message and get back to you shortly.

    Regards,
    Support Team
    """

    response_text = response_templates.get(predicted_class, default_response)

    return response_text


input_text = input("Input text description: ")
user_name = input("Enter user's name: ")
ticket_subject = input("Enter ticket subject: ")
email = input("Enter user's email: ")

print("\nAuto-Response:\n")
print(auto_response(input_text, user_name, ticket_subject, email))

